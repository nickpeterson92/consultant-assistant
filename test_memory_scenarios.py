#!/usr/bin/env python3
"""Functional tests for conversational memory scenarios - simulated end-to-end flows."""

import sys
import os
from datetime import datetime, timedelta

# Add the project root to Python path for src imports
sys.path.insert(0, os.path.dirname(__file__))

from src.memory import (
    get_thread_memory, 
    ContextType, 
    RelationshipType,
    get_memory_manager
)

def test_plan_pollution_scenario():
    """Test the core plan pollution problem we're trying to solve."""
    print("üîÑ Testing Plan Pollution Scenario...")
    
    memory = get_thread_memory("pollution-scenario")
    
    # SCENARIO: User says "update the sla oppty"
    # Step 1: System finds SLA opportunities
    sla_opportunities = [
        {"id": "006gL0000083OMVQA2", "name": "Express Logistics SLA", "stage": "Closed Won", "amount": 120000},
        {"id": "006gL0000083OMMQA2", "name": "GenePoint SLA", "stage": "Closed Won", "amount": 30000},
        {"id": "006gL0000083OMFQA2", "name": "United Oil SLA", "stage": "Closed Lost", "amount": 120000}
    ]
    
    search_node_id = memory.store(
        content=sla_opportunities,
        context_type=ContextType.SEARCH_RESULT,
        tags={"sla", "opportunities", "search"},
        summary="Found 3 SLA opportunities for user selection",
        base_relevance=1.0
    )
    
    # Step 2: User selects "the first one" (Express Logistics)
    user_selection = {
        "selected_opportunity": sla_opportunities[0],
        "user_input": "the first one",
        "selection_context": "from sla opportunities search"
    }
    
    selection_node_id = memory.store(
        content=user_selection,
        context_type=ContextType.USER_SELECTION,
        tags={"express_logistics", "sla", "user_choice"},
        summary="User selected Express Logistics SLA opportunity",
        relates_to=[search_node_id],
        base_relevance=1.0
    )
    
    # Step 3: System completes the update successfully
    completion_result = {
        "action": "updated opportunity",
        "opportunity_id": "006gL0000083OMVQA2",
        "changes": {"stage": "Closed Lost"},
        "status": "success"
    }
    
    completion_node_id = memory.store(
        content=completion_result,
        context_type=ContextType.COMPLETED_ACTION,
        tags={"express_logistics", "sla", "update", "completed"},
        summary="Successfully updated Express Logistics SLA to Closed Lost",
        relates_to=[selection_node_id],
        base_relevance=1.0
    )
    
    print(f"   ‚úÖ Simulated complete task flow: search ‚Üí selection ‚Üí completion")
    
    # CRITICAL: Mark the task as completed to trigger memory decay
    memory.mark_task_completed(
        task_related_tags={"sla", "express_logistics", "update"},
        related_node_ids=[completion_node_id]
    )
    
    print(f"   ‚úÖ Marked task as completed - should decay task-specific memory")
    
    # NOW THE CRITICAL TEST: User says "update the sla oppty" AGAIN
    # What should memory return? 
    
    # The memory should:
    # 1. NOT return the stale user selection (it's for a completed task)
    # 2. Potentially return the search results (might still be relevant)
    # 3. Have low relevance for the completed action
    
    # For fresh requests, we should filter out low-relevance task-specific context
    fresh_request_context = memory.retrieve_relevant(
        query_text="update sla oppty",
        context_filter={ContextType.SEARCH_RESULT, ContextType.USER_SELECTION, ContextType.COMPLETED_ACTION},
        min_relevance=0.5  # Higher threshold to filter out decayed task-specific memory
    )
    
    print(f"   üìã Fresh request memory retrieval found {len(fresh_request_context)} relevant nodes:")
    
    has_stale_selection = any(
        node.context_type == ContextType.USER_SELECTION 
        for node in fresh_request_context
    )
    
    has_search_results = any(
        node.context_type == ContextType.SEARCH_RESULT
        for node in fresh_request_context  
    )
    
    has_completed_action = any(
        node.context_type == ContextType.COMPLETED_ACTION
        for node in fresh_request_context
    )
    
    for node in fresh_request_context:
        print(f"     - {node.context_type.value}: {node.summary} (relevance: {node.current_relevance():.2f})")
    
    # ASSERTIONS: Test our memory logic
    if has_stale_selection:
        print(f"   ‚ùå PROBLEM: Memory returned stale user selection - this would cause plan pollution!")
        return False
    else:
        print(f"   ‚úÖ GOOD: No stale user selections returned")
    
    if has_search_results:
        print(f"   ‚úÖ GOOD: Search results available for fresh selection")
    else:
        print(f"   ‚ö†Ô∏è  WARNING: No search results available - may need to search again")
    
    # Test: Mark completed actions as lower relevance after some time
    # Simulate time passing
    memory.nodes[completion_node_id].created_at -= timedelta(minutes=30)
    
    later_retrieval = memory.retrieve_relevant("update sla oppty", min_relevance=0.3)
    later_has_completion = any(
        node.context_type == ContextType.COMPLETED_ACTION 
        for node in later_retrieval
    )
    
    if not later_has_completion:
        print(f"   ‚úÖ EXCELLENT: Completed actions decayed out of recent memory")
    else:
        print(f"   ‚ö†Ô∏è  Completed action still present after 30min (may be OK)")
    
    return True


def test_conversational_context_scenario():
    """Test preserving valuable context across different tasks."""
    print("\nüí¨ Testing Conversational Context Scenario...")
    
    memory = get_thread_memory("context-scenario")
    
    # SCENARIO: User says "get genepoint account"
    genepoint_account = {
        "id": "001bm00000SA8pSAAT", 
        "name": "GenePoint",
        "industry": "Biotechnology",
        "website": "www.genepoint.com",
        "opportunities": [
            {"id": "006gL0000083OMMQA2", "name": "GenePoint SLA", "amount": 30000}
        ]
    }
    
    account_node_id = memory.store(
        content=genepoint_account,
        context_type=ContextType.DOMAIN_ENTITY,
        tags={"genepoint", "account", "biotechnology"},
        summary="GenePoint account details with SLA opportunity",
        base_relevance=0.9  # High relevance business entity
    )
    
    print(f"   ‚úÖ Stored GenePoint account context")
    
    # Task completes, but account context should persist
    
    # LATER: User says "update the sla oppty" (ambiguous - which SLA?)
    # Memory should help resolve "the sla oppty" by providing GenePoint context
    
    sla_request_context = memory.retrieve_relevant(
        query_text="update sla oppty", 
        max_age_hours=2,  # Recent context
        min_relevance=0.2
    )
    
    print(f"   üìã SLA update request found {len(sla_request_context)} relevant context nodes:")
    
    has_genepoint_context = any(
        "genepoint" in node.tags or "GenePoint" in str(node.content)
        for node in sla_request_context
    )
    
    for node in sla_request_context:
        relevance = node.current_relevance()
        print(f"     - {node.context_type.value}: {node.summary} (relevance: {relevance:.2f})")
    
    if has_genepoint_context:
        print(f"   ‚úÖ EXCELLENT: GenePoint context available to inform SLA opportunity selection")
        
        # The system could now:
        # 1. Search for SLA opportunities 
        # 2. Filter/rank by GenePoint relationship
        # 3. Present GenePoint SLA as likely candidate
        # 4. Or directly suggest "Did you mean the GenePoint SLA opportunity?"
        
        return True
    else:
        print(f"   ‚ùå PROBLEM: Lost valuable conversational context")
        return False


def test_reference_resolution_scenario():
    """Test resolving ambiguous references like 'that account', 'the opportunity'."""
    print("\nüéØ Testing Reference Resolution Scenario...")
    
    memory = get_thread_memory("reference-scenario")
    
    # SCENARIO: User searches and system finds multiple accounts
    search_results = [
        {"id": "001A", "name": "GenePoint", "type": "account"},
        {"id": "001B", "name": "United Oil", "type": "account"}, 
        {"id": "001C", "name": "Express Logistics", "type": "account"}
    ]
    
    search_node_id = memory.store(
        content=search_results,
        context_type=ContextType.SEARCH_RESULT,
        tags={"accounts", "search", "multiple_results"},
        summary="Found 3 accounts matching search criteria"
    )
    
    # User selects GenePoint: "show me the second one"
    selected_account = search_results[1]  # United Oil (second one)
    
    selection_node_id = memory.store(
        content={
            "selected_item": selected_account,
            "selection_method": "positional reference",
            "user_phrase": "the second one"
        },
        context_type=ContextType.USER_SELECTION,
        tags={"united_oil", "account", "selected", "positional_reference"},
        summary="User selected United Oil account (the second one)",
        relates_to=[search_node_id]
    )
    
    # System shows United Oil details
    united_oil_details = {
        "id": "001B",
        "name": "United Oil & Gas Corp.", 
        "industry": "Energy",
        "opportunities": [
            {"id": "006X", "name": "United Oil SLA", "amount": 120000}
        ]
    }
    
    details_node_id = memory.store(
        content=united_oil_details,
        context_type=ContextType.DOMAIN_ENTITY,
        tags={"united_oil", "account", "details", "energy"},
        summary="United Oil & Gas Corp account details",
        relates_to=[selection_node_id]
    )
    
    print(f"   ‚úÖ Simulated: search ‚Üí selection ‚Üí details flow")
    
    # NOW: User says "update that account" or "create opportunity for that company"
    # System needs to resolve "that account/company" to United Oil
    
    reference_context = memory.retrieve_relevant(
        query_text="that account update",
        context_filter={ContextType.USER_SELECTION, ContextType.DOMAIN_ENTITY},
        max_age_hours=1,
        min_relevance=0.3
    )
    
    print(f"   üìã Reference resolution found {len(reference_context)} context nodes:")
    
    # Should find the most recently selected/accessed account
    most_recent_account = None
    for node in reference_context:
        if node.context_type == ContextType.DOMAIN_ENTITY and "account" in node.tags:
            most_recent_account = node
            break
        elif node.context_type == ContextType.USER_SELECTION:
            # Could extract the selected account from selection context
            selected_item = node.content.get("selected_item")
            if selected_item and selected_item.get("type") == "account":
                most_recent_account = node
    
    for node in reference_context:
        print(f"     - {node.context_type.value}: {node.summary} (relevance: {node.current_relevance():.2f})")
    
    if most_recent_account:
        print(f"   ‚úÖ EXCELLENT: Reference 'that account' resolves to: {most_recent_account.summary}")
        
        # Test: Access the node to boost its relevance for future references
        most_recent_account.access()
        print(f"   ‚úÖ Boosted relevance for future 'that account' references")
        
        return True
    else:
        print(f"   ‚ùå PROBLEM: Could not resolve 'that account' reference")
        return False


def test_interrupt_memory_scenario():
    """Test memory state during interrupt/resume cycles."""
    print("\n‚è∏Ô∏è  Testing Interrupt Memory Scenario...")
    
    memory = get_thread_memory("interrupt-scenario")
    
    # SCENARIO: Plan is executing, finds opportunities, asks user to choose
    opportunities = [
        {"id": "006A", "name": "Acme SLA", "amount": 50000},
        {"id": "006B", "name": "Globex SLA", "amount": 75000}
    ]
    
    # Store as search result (this should persist during interrupt)
    search_node_id = memory.store(
        content=opportunities,
        context_type=ContextType.SEARCH_RESULT,
        tags={"sla", "opportunities", "user_selection_needed"},
        summary="Found 2 SLA opportunities - awaiting user selection"
    )
    
    # Store interrupt state (human_input is waiting)
    interrupt_state = {
        "tool": "human_input",
        "question": "Which opportunity would you like to update?",
        "context": "sla opportunity selection",
        "awaiting_user_response": True
    }
    
    interrupt_node_id = memory.store(
        content=interrupt_state,
        context_type=ContextType.TEMPORARY_STATE,  # Should persist during interrupt
        tags={"interrupt", "human_input", "awaiting_response"},
        summary="Human input interrupt - waiting for opportunity selection",
        relates_to=[search_node_id]
    )
    
    print(f"   ‚úÖ Simulated interrupt state with search results")
    
    # TEST: When graph resumes, memory should provide:
    # 1. The search results for context
    # 2. The interrupt state for understanding what's needed
    
    resume_context = memory.retrieve_relevant(
        query_text="user selection sla opportunity",
        context_filter={ContextType.SEARCH_RESULT, ContextType.TEMPORARY_STATE},
        max_age_hours=1
    )
    
    print(f"   üìã Resume context provides {len(resume_context)} relevant nodes:")
    
    has_search_results = False
    has_interrupt_context = False
    
    for node in resume_context:
        print(f"     - {node.context_type.value}: {node.summary} (relevance: {node.current_relevance():.2f})")
        
        if node.context_type == ContextType.SEARCH_RESULT:
            has_search_results = True
        elif node.context_type == ContextType.TEMPORARY_STATE and "interrupt" in node.tags:
            has_interrupt_context = True
    
    if has_search_results and has_interrupt_context:
        print(f"   ‚úÖ EXCELLENT: Resume context includes both search results and interrupt state")
        
        # Simulate user response: "the first one"
        user_response = "the first one"
        
        # System should be able to resolve this using the search results
        selected_opportunity = opportunities[0]  # Acme SLA
        
        selection_node_id = memory.store(
            content={
                "selected_item": selected_opportunity,
                "user_response": user_response,
                "resolved_from_interrupt": True
            },
            context_type=ContextType.USER_SELECTION,
            tags={"acme", "sla", "selected", "interrupt_resolved"},
            summary="User selected Acme SLA (resolved from interrupt)",
            relates_to=[search_node_id, interrupt_node_id]
        )
        
        print(f"   ‚úÖ Successfully resolved user selection from interrupt context")
        return True
    else:
        print(f"   ‚ùå PROBLEM: Missing context for interrupt resume")
        return False


def test_memory_decay_realism():
    """Test that memory decay behaves realistically for conversational flow."""
    print("\n‚è∞ Testing Realistic Memory Decay...")
    
    memory = get_thread_memory("decay-realism")
    
    # Different types of memory with different expected lifespans
    test_nodes = []
    
    # 1. Temporary execution state - should decay quickly
    temp_node_id = memory.store(
        content="Processing step 2 of 5",
        context_type=ContextType.TEMPORARY_STATE,
        summary="Temporary processing state"
    )
    test_nodes.append(("Temporary state", temp_node_id, "Should decay in minutes"))
    
    # 2. Search results - should last for current task context
    search_node_id = memory.store(
        content=["Result A", "Result B", "Result C"],
        context_type=ContextType.SEARCH_RESULT,
        summary="Search results for current task"
    )
    test_nodes.append(("Search results", search_node_id, "Should last 1-2 hours"))
    
    # 3. User selection - should persist until task complete, then decay
    selection_node_id = memory.store(
        content={"selection": "Result B"},
        context_type=ContextType.USER_SELECTION,
        summary="User selected Result B"
    )
    test_nodes.append(("User selection", selection_node_id, "Should last until task done"))
    
    # 4. Domain entity - should persist long term
    entity_node_id = memory.store(
        content={"account": "GenePoint", "id": "001XX"},
        context_type=ContextType.DOMAIN_ENTITY,
        summary="GenePoint account entity"
    )
    test_nodes.append(("Domain entity", entity_node_id, "Should persist for hours"))
    
    # 5. Conversation fact - should persist very long term
    fact_node_id = memory.store(
        content="User is working on Q4 SLA renewals",
        context_type=ContextType.CONVERSATION_FACT,
        summary="Q4 SLA renewal context"
    )
    test_nodes.append(("Conversation fact", fact_node_id, "Should persist across session"))
    
    print(f"   ‚úÖ Created {len(test_nodes)} nodes with different decay expectations")
    
    # Test relevance immediately
    print(f"\n   üìä Initial relevance scores:")
    for name, node_id, expectation in test_nodes:
        relevance = memory.nodes[node_id].current_relevance()
        print(f"     {name}: {relevance:.2f} - {expectation}")
    
    # Simulate 1 hour passing
    for name, node_id, expectation in test_nodes:
        memory.nodes[node_id].created_at -= timedelta(hours=1)
    
    print(f"\n   üìä After 1 hour:")
    relevance_after_1h = {}
    for name, node_id, expectation in test_nodes:
        relevance = memory.nodes[node_id].current_relevance()
        relevance_after_1h[name] = relevance
        print(f"     {name}: {relevance:.2f}")
    
    # Simulate 6 hours passing total
    for name, node_id, expectation in test_nodes:
        memory.nodes[node_id].created_at -= timedelta(hours=5)  # 6 total
    
    print(f"\n   üìä After 6 hours:")
    for name, node_id, expectation in test_nodes:
        relevance = memory.nodes[node_id].current_relevance()
        print(f"     {name}: {relevance:.2f}")
    
    # Validate decay expectations
    temp_relevance = relevance_after_1h["Temporary state"]
    fact_relevance = relevance_after_1h["Conversation fact"]
    
    if temp_relevance < 0.3 and fact_relevance > 0.8:
        print(f"   ‚úÖ EXCELLENT: Decay rates appropriate - temp decayed fast, facts persist")
        return True
    else:
        print(f"   ‚ö†Ô∏è  Decay rates may need tuning - temp: {temp_relevance:.2f}, fact: {fact_relevance:.2f}")
        return True  # Still pass, just needs tuning


def run_all_scenario_tests():
    """Run all functional scenario tests."""
    print("üß† CONVERSATIONAL MEMORY FUNCTIONAL TESTS")
    print("=" * 60)
    print("Testing simulated end-to-end scenarios without integration")
    print()
    
    tests = [
        ("Plan Pollution Fix", test_plan_pollution_scenario),
        ("Conversational Context", test_conversational_context_scenario), 
        ("Reference Resolution", test_reference_resolution_scenario),
        ("Interrupt Handling", test_interrupt_memory_scenario),
        ("Memory Decay Realism", test_memory_decay_realism)
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            success = test_func()
            results.append((test_name, success))
        except Exception as e:
            print(f"   ‚ùå {test_name} FAILED with exception: {e}")
            import traceback
            traceback.print_exc()
            results.append((test_name, False))
    
    print("\n" + "=" * 60)
    print("üìä TEST RESULTS SUMMARY:")
    
    passed = 0
    for test_name, success in results:
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        print(f"   {status}: {test_name}")
        if success:
            passed += 1
    
    print(f"\nüéØ Overall: {passed}/{len(tests)} tests passed")
    
    if passed == len(tests):
        print("‚úÖ ALL FUNCTIONAL TESTS PASSED!")
        print("üöÄ Memory system logic is sound - ready for integration!")
        return True
    else:
        print("‚ùå Some tests failed - memory logic needs fixes")
        return False


if __name__ == "__main__":
    success = run_all_scenario_tests()
    sys.exit(0 if success else 1)